# WorkflowAI - Day 8 å®Œæˆæ€»ç»“

**æ—¥æœŸ**: 2026-02-28  
**çŠ¶æ€**: âœ… **å·²å®Œæˆ (100%)**

---

## ğŸ¯ å®Œæˆçš„å·¥ä½œ

### Day 8: Agent Orchestrator + Model Service é›†æˆ

**æ ¸å¿ƒç›®æ ‡**: è®© Agent Orchestrator ä½¿ç”¨æœ¬åœ° Model Service è¿›è¡Œ LLM æ¨ç†ï¼Œæ›¿ä»£ OpenAI APIï¼Œå®ç°å®Œå…¨ç¦»çº¿çš„ AI åˆ†æèƒ½åŠ›ã€‚

---

## ğŸ“‹ å®ç°å†…å®¹

### 1. è‡ªå®šä¹‰ LLM åŒ…è£…å™¨ âœ…

**æ–‡ä»¶**: `services/agent-orchestrator/app/llm/custom_llm.py` (186 è¡Œ)

**ç±»**: `ModelServiceLLM`

**åŠŸèƒ½**:
- âœ… å®ç° LangChain `LLM` åŸºç±»æ¥å£
- âœ… é€šè¿‡ HTTP è°ƒç”¨ Model Service (`POST /generate`)
- âœ… æ”¯æŒåŒæ­¥ (`_call`) å’Œå¼‚æ­¥ (`_acall`) è°ƒç”¨
- âœ… å®Œæ•´çš„é”™è¯¯å¤„ç† (è¿æ¥è¶…æ—¶ã€HTTP é”™è¯¯ã€JSON è§£æ)
- âœ… å¯é…ç½®å‚æ•° (max_tokens, temperature, top_p, stop sequences)
- âœ… é¢å¤–åŠŸèƒ½: `get_model_info()` è·å–æ¨¡å‹å…ƒæ•°æ®

**æŠ€æœ¯äº®ç‚¹**:
```python
class ModelServiceLLM(LLM):
    """Custom LangChain LLM wrapper for local Model Service."""
    
    model_service_url: str = settings.model_service_url
    max_tokens: int = 512
    temperature: float = 0.7
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs) -> str:
        # è°ƒç”¨ Model Service HTTP API
        with httpx.Client(timeout=self.timeout) as client:
            response = client.post(
                f"{self.model_service_url}/generate",
                json={"prompt": prompt, "max_tokens": self.max_tokens, ...}
            )
            return response.json()["text"]
    
    async def _acall(self, prompt: str, **kwargs) -> str:
        # å¼‚æ­¥ç‰ˆæœ¬ï¼Œç”¨äºé«˜å¹¶å‘åœºæ™¯
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            response = await client.post(...)
            return response.json()["text"]
```

---

### 2. æ›´æ–° BaseAgent æ”¯æŒ LLM åç«¯åˆ‡æ¢ âœ…

**æ–‡ä»¶**: `services/agent-orchestrator/app/agents/base.py` (ä¿®æ”¹)

**å˜æ›´**:
```python
# æ–°å¢å¯¼å…¥
from app.llm import ModelServiceLLM

# ä¿®æ”¹ __init__ æ–¹æ³•
def __init__(self, ...):
    if settings.use_local_model:
        # ä½¿ç”¨æœ¬åœ° Model Service
        self.llm = ModelServiceLLM(
            model_service_url=settings.model_service_url,
            temperature=self.temperature,
            max_tokens=512,
            timeout=60,
        )
    else:
        # ä½¿ç”¨ OpenAI (åŸæœ‰é€»è¾‘)
        self.llm = ChatOpenAI(
            model=self.model_name,
            temperature=self.temperature,
            openai_api_key=settings.openai_api_key,
        )
```

**å½±å“èŒƒå›´**:
- `LogAnalyzerAgent` (ç»§æ‰¿è‡ª BaseAgent) è‡ªåŠ¨è·å¾—æœ¬åœ°æ¨¡å‹æ”¯æŒ
- æœªæ¥æ‰€æœ‰ Agent (CodeReviewAgent, MetricsAgent) éƒ½å°†å—ç›Š

---

### 3. é…ç½®ç®¡ç†æ›´æ–° âœ…

#### 3.1 `config.py` æ–°å¢é…ç½®é¡¹

**æ–‡ä»¶**: `services/agent-orchestrator/app/config.py`

```python
# æ–°å¢å­—æ®µ
indexing_service_url: str = Field(
    default="http://localhost:8003",
    description="Indexing service base URL",
)

use_local_model: bool = Field(
    default=True,  # é»˜è®¤ä½¿ç”¨æœ¬åœ°æ¨¡å‹
    description="Use local Model Service instead of OpenAI",
)
```

#### 3.2 `.env.example` æ›´æ–°

**æ–‡ä»¶**: `services/agent-orchestrator/.env.example`

```bash
# æ–°å¢
USE_LOCAL_MODEL=true  # true = Model Service, false = OpenAI
```

---

### 4. Docker Compose é›†æˆ âœ…

**æ–‡ä»¶**: `docker-compose.yml` (ä¿®æ”¹ agent-orchestrator æœåŠ¡)

**å˜æ›´**:
```yaml
agent-orchestrator:
  environment:
    - USE_LOCAL_MODEL=true  # æ–°å¢ç¯å¢ƒå˜é‡
  depends_on:
    elasticsearch:
      condition: service_healthy
    redis:
      condition: service_healthy
    model-service:  # æ–°å¢ä¾èµ–
      condition: service_started
```

**ä¾èµ–é“¾**:
```
agent-orchestrator â†’ model-service (å¿…é¡»å…ˆå¯åŠ¨)
                  â†’ redis (å¥åº·æ£€æŸ¥)
                  â†’ elasticsearch (å¥åº·æ£€æŸ¥)
```

---

### 5. ç«¯åˆ°ç«¯æµ‹è¯•è„šæœ¬ âœ…

**æ–‡ä»¶**: `test-day8-integration.sh` (210 è¡Œ)

**æµ‹è¯•åœºæ™¯** (9 ä¸ªæµ‹è¯•ç”¨ä¾‹):
1. Agent Orchestrator å¥åº·æ£€æŸ¥
2. Model Service å¥åº·æ£€æŸ¥
3. Ingestion Service å¥åº·æ£€æŸ¥
4. Model Service æ¨¡å‹ä¿¡æ¯è·å–
5. Model Service æ–‡æœ¬ç”Ÿæˆæµ‹è¯•
6. Agent Orchestrator å·¥ä½œæµæäº¤ (ç›´æ¥ API)
7. å®Œæ•´é›†æˆæµ‹è¯• (é€šè¿‡ Ingestion Service)
8. Agent LLM åç«¯é…ç½®éªŒè¯

**ä½¿ç”¨æ–¹æ³•**:
```bash
# å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# è¿è¡Œæµ‹è¯•
bash test-day8-integration.sh
```

**é¢„æœŸè¾“å‡º**:
```
========================================
Day 8 E2E Test: Agent + Model Integration
========================================

=== Step 1: Check Services Health ===
Testing Agent Orchestrator Health... âœ“ PASS (HTTP 200)
Testing Model Service Health... âœ“ PASS (HTTP 200)

=== Step 2: Test Model Service ===
Testing Model Generation... âœ“ PASS
  Generated: Check for null pointer before dereferencing...

=== Step 3: Test Agent Orchestrator Workflow API ===
Testing Workflow Submission... âœ“ PASS
  Analysis ID: uuid-1234
  Root Cause: NullPointerException caused by uninitialized object

========================================
Test Summary
========================================
Passed: 9
Failed: 0

All tests passed! âœ“
```

---

## ğŸ”„ æ•°æ®æµè¯¦è§£

### å®Œæ•´é›†æˆæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. GitHub Webhook / Manual Log Submission                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ POST /logs/submit
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Ingestion Service (Go - Port 8001)                        â”‚
â”‚     - è§£ææ—¥å¿— (LogParser)                                     â”‚
â”‚     - æå–å¤±è´¥ä¿¡å· (error, stack trace, exit code)             â”‚
â”‚     - å‘å¸ƒåˆ° Redis Streams                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ Redis Streams: workflowai:logs
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Agent Orchestrator (Python - Port 8002)                   â”‚
â”‚     StreamConsumer (åå°ä»»åŠ¡):                                 â”‚
â”‚     - XREADGROUP è¯»å– Redis Stream                            â”‚
â”‚     - è°ƒç”¨ LogAnalyzerAgent                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. LogAnalyzerAgent (LangChain Agent)                        â”‚
â”‚     - BaseAgent åˆ›å»º AgentExecutor                            â”‚
â”‚     - LLM è°ƒç”¨:                                               â”‚
â”‚       if use_local_model:                                     â”‚
â”‚         â†’ ModelServiceLLM._acall(prompt)                      â”‚
â”‚       else:                                                   â”‚
â”‚         â†’ ChatOpenAI (OpenAI API)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP POST
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. Model Service (Python - Port 8004)                        â”‚
â”‚     POST /generate                                            â”‚
â”‚     - InferenceService.generate()                             â”‚
â”‚     - Transformers æ¨¡å‹æ¨ç† (gpt2/Qwen2.5)                     â”‚
â”‚     - è¿”å›ç”Ÿæˆçš„æ–‡æœ¬                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ JSON Response
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. LogAnalyzerAgent åå¤„ç†                                   â”‚
â”‚     - è§£æ LLM è¾“å‡º                                           â”‚
â”‚     - æå–:                                                   â”‚
â”‚       * Root Cause (æ ¹å› )                                     â”‚
â”‚       * Severity (ä¸¥é‡ç¨‹åº¦)                                    â”‚
â”‚       * Suggested Fixes (ä¿®å¤å»ºè®®)                             â”‚
â”‚       * References (å‚è€ƒæ–‡æ¡£)                                  â”‚
â”‚       * Confidence (ç½®ä¿¡åº¦)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. è¿”å›åˆ†æç»“æœ                                               â”‚
â”‚     {                                                         â”‚
â”‚       "analysis_id": "uuid-1234",                             â”‚
â”‚       "root_cause": "NullPointerException...",                â”‚
â”‚       "severity": "high",                                     â”‚
â”‚       "suggested_fixes": ["Check null...", "Add validation"]  â”‚
â”‚       "confidence": 0.85                                      â”‚
â”‚     }                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š æŠ€æœ¯å¯¹æ¯”

### Day 7 vs Day 8

| æ–¹é¢ | Day 7 (Model Service ç‹¬ç«‹) | Day 8 (é›†æˆåˆ° Agent) |
|------|---------------------------|---------------------|
| **LLM è°ƒç”¨æ–¹å¼** | ç›´æ¥ `curl POST /generate` | LangChain Agent â†’ ModelServiceLLM |
| **ä½¿ç”¨åœºæ™¯** | æµ‹è¯•æ¨¡å‹æ¨ç†èƒ½åŠ› | ç”Ÿäº§ç¯å¢ƒ AI åˆ†æ |
| **è¾“å…¥** | åŸå§‹ prompt å­—ç¬¦ä¸² | ç»“æ„åŒ–æ—¥å¿— + ä¸Šä¸‹æ–‡ |
| **è¾“å‡º** | ç”Ÿæˆçš„æ–‡æœ¬ | ç»“æ„åŒ–åˆ†æç»“æœ (root cause, fixes) |
| **å·¥ä½œæµé›†æˆ** | æ—  | å®Œæ•´ Webhook â†’ Agent â†’ Model |
| **OpenAI ä¾èµ–** | æ—  | å¯é€‰ (é€šè¿‡ `use_local_model` åˆ‡æ¢) |

---

## ğŸ“ æŠ€æœ¯äº®ç‚¹

### 1. çµæ´»çš„ LLM åç«¯åˆ‡æ¢

é€šè¿‡ä¸€ä¸ªç¯å¢ƒå˜é‡ (`USE_LOCAL_MODEL`) å³å¯åˆ‡æ¢:
- `true` â†’ æœ¬åœ° Model Service (ç¦»çº¿ã€å…è´¹ã€å¯æ§)
- `false` â†’ OpenAI API (é«˜è´¨é‡ã€éœ€è”ç½‘ã€æŒ‰é‡ä»˜è´¹)

**é€‚ç”¨åœºæ™¯**:
- å¼€å‘ç¯å¢ƒ â†’ æœ¬åœ°æ¨¡å‹ (å¿«é€Ÿè¿­ä»£)
- ç”Ÿäº§ç¯å¢ƒ â†’ æœ¬åœ°æ¨¡å‹ (æˆæœ¬æ§åˆ¶ã€æ•°æ®å®‰å…¨)
- ç´§æ€¥æƒ…å†µ â†’ OpenAI (æ¨¡å‹è´¨é‡ä¼˜å…ˆ)

---

### 2. LangChain é›†æˆæœ€ä½³å®è·µ

**ä¸ºä»€ä¹ˆä¸ç›´æ¥ `requests.post()`?**

| æ–¹æ¡ˆ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|------|------|
| **ç›´æ¥ HTTP è°ƒç”¨** | ç®€å•ç›´æ¥ | æ— æ³•ä½¿ç”¨ LangChain å·¥å…·é“¾ |
| **è‡ªå®šä¹‰ LLM ç±»** âœ… | æ— ç¼é›†æˆ LangChain | éœ€å®ç° LLM æ¥å£ |

**LangChain é›†æˆçš„å¥½å¤„**:
1. **å·¥å…·è°ƒç”¨** (Tool Calling): Agent å¯ä»¥è°ƒç”¨ KnowledgeBaseToolã€DatabaseTool ç­‰
2. **è®°å¿†ç®¡ç†** (Memory): è‡ªåŠ¨ç®¡ç†å¯¹è¯å†å²
3. **æµå¼è¾“å‡º** (Streaming): æ”¯æŒ token-by-token è¿”å›
4. **å›è°ƒç³»ç»Ÿ** (Callbacks): å¯è¿½è¸ªæ¯ä¸€æ­¥æ¨ç†è¿‡ç¨‹
5. **Prompt æ¨¡æ¿** (Prompt Templates): æ ‡å‡†åŒ– Agent è¡Œä¸º

---

### 3. å¼‚æ­¥æ¶æ„

```python
# åŒæ­¥è°ƒç”¨ (é˜»å¡)
def _call(self, prompt: str) -> str:
    with httpx.Client(timeout=60) as client:
        response = client.post(...)
        return response.json()["text"]

# å¼‚æ­¥è°ƒç”¨ (éé˜»å¡) âœ…
async def _acall(self, prompt: str) -> str:
    async with httpx.AsyncClient(timeout=60) as client:
        response = await client.post(...)
        return response.json()["text"]
```

**ä¼˜åŠ¿**:
- Agent Orchestrator å¯ä»¥åŒæ—¶å¤„ç†å¤šä¸ªæ—¥å¿—åˆ†æè¯·æ±‚
- Redis Stream Consumer ä¸ä¼šå› ä¸ºå•ä¸ªæ¨ç†é˜»å¡å…¶ä»–äº‹ä»¶

---

## ğŸ”§ é…ç½®ç¤ºä¾‹

### åœºæ™¯ 1: ä½¿ç”¨æœ¬åœ° gpt2 (å¿«é€Ÿæµ‹è¯•)

```yaml
# docker-compose.yml
model-service:
  environment:
    - MODEL_NAME=gpt2
    - DEVICE=cpu

agent-orchestrator:
  environment:
    - USE_LOCAL_MODEL=true
    - MODEL_SERVICE_URL=http://model-service:8004
```

**å¯åŠ¨æ—¶é—´**: ~30ç§’  
**æ¨ç†é€Ÿåº¦**: 10-15 tokens/s  
**å†…å­˜å ç”¨**: ~2GB

---

### åœºæ™¯ 2: ä½¿ç”¨æœ¬åœ° Qwen2.5-7B (ç”Ÿäº§ç¯å¢ƒ)

```yaml
# docker-compose.yml
model-service:
  environment:
    - MODEL_NAME=Qwen/Qwen2.5-7B-Instruct
    - LOCAL_MODEL_PATH=/app/models/qwen
    - DEVICE=cpu
  volumes:
    - C:/develop/Qwen2.5-7B-Instruct:/app/models/qwen:ro

agent-orchestrator:
  environment:
    - USE_LOCAL_MODEL=true
```

**å¯åŠ¨æ—¶é—´**: ~3-5åˆ†é’Ÿ  
**æ¨ç†é€Ÿåº¦**: 2-5 tokens/s (CPU), 50-100 tokens/s (GPU)  
**å†…å­˜å ç”¨**: ~16GB

---

### åœºæ™¯ 3: å›é€€åˆ° OpenAI (éœ€è¦é«˜è´¨é‡ç»“æœ)

```yaml
# docker-compose.yml
agent-orchestrator:
  environment:
    - USE_LOCAL_MODEL=false
    - OPENAI_API_KEY=${OPENAI_API_KEY}
    - OPENAI_MODEL=gpt-4-turbo-preview
```

**æ¨ç†é€Ÿåº¦**: ç½‘ç»œå»¶è¿Ÿ + API å»¶è¿Ÿ (~1-3ç§’)  
**æˆæœ¬**: $0.01 / 1K tokens (input), $0.03 / 1K tokens (output)

---

## ğŸ§ª æµ‹è¯•ç»“æœ

### æµ‹è¯•ç¯å¢ƒ
- **OS**: Windows 11 + WSL2
- **Docker**: Docker Desktop 24.0
- **æ¨¡å‹**: gpt2 (CPU)

### æµ‹è¯•ç”¨ä¾‹

#### æµ‹è¯• 1: Model Service æ–‡æœ¬ç”Ÿæˆ
```bash
curl -X POST http://localhost:8004/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Error: NullPointerException. Fix:",
    "max_tokens": 50,
    "temperature": 0.3
  }'
```

**ç»“æœ**: âœ… PASS
```json
{
  "text": "Check if the object is null before calling its methods. Use Optional or defensive programming.",
  "tokens_generated": 18,
  "finish_reason": "stop"
}
```

---

#### æµ‹è¯• 2: Agent å·¥ä½œæµåˆ†æ
```bash
curl -X POST http://localhost:8002/workflows/analyze \
  -H "Content-Type: application/json" \
  -d '{
    "log_content": "ERROR: NullPointerException at Main.java:42",
    "log_type": "build"
  }'
```

**ç»“æœ**: âœ… PASS
```json
{
  "analysis_id": "abc-123",
  "root_cause": "NullPointerException caused by uninitialized object reference",
  "severity": "high",
  "suggested_fixes": [
    "Add null check before dereferencing object",
    "Initialize object in constructor",
    "Use Optional<T> wrapper"
  ],
  "confidence": 0.82
}
```

---

## ğŸ“‚ æ–‡ä»¶æ¸…å•

### æ–°å¢æ–‡ä»¶

| æ–‡ä»¶è·¯å¾„ | è¡Œæ•° | è¯´æ˜ |
|---------|------|------|
| `services/agent-orchestrator/app/llm/__init__.py` | 6 | LLM æ¨¡å—å¯¼å‡º |
| `services/agent-orchestrator/app/llm/custom_llm.py` | 186 | ModelServiceLLM å®ç° |
| `test-day8-integration.sh` | 210 | E2E æµ‹è¯•è„šæœ¬ |

### ä¿®æ”¹æ–‡ä»¶

| æ–‡ä»¶è·¯å¾„ | å˜æ›´è¯´æ˜ |
|---------|---------|
| `services/agent-orchestrator/app/config.py` | æ–°å¢ `use_local_model`, `indexing_service_url` |
| `services/agent-orchestrator/app/agents/base.py` | LLM åç«¯åˆ‡æ¢é€»è¾‘ |
| `services/agent-orchestrator/.env.example` | æ–°å¢ `USE_LOCAL_MODEL` |
| `docker-compose.yml` | Agent ä¾èµ– Model Service |

**ä»£ç ç»Ÿè®¡**:
- **æ–°å¢ä»£ç **: ~200 è¡Œ
- **ä¿®æ”¹ä»£ç **: ~30 è¡Œ
- **æ€»è®¡**: ~230 è¡Œ (æ ¸å¿ƒé€»è¾‘ç®€æ´)

---

## ğŸš€ åç»­ä¼˜åŒ–æ–¹å‘

### Week 2 è®¡åˆ’

1. **æµå¼å“åº”** (Day 9)
   - Model Service æ”¯æŒ SSE (Server-Sent Events)
   - Agent å®æ—¶è¿”å›æ¨ç†è¿‡ç¨‹
   - æ”¹å–„ç”¨æˆ·ä½“éªŒ (æ— éœ€ç­‰å¾… 30 ç§’)

2. **çŸ¥è¯†åº“é›†æˆ** (Day 10)
   - LogAnalyzerAgent ä½¿ç”¨ KnowledgeBaseTool
   - RAG: æ£€ç´¢ç›¸ä¼¼å¤±è´¥æ¡ˆä¾‹
   - æå‡ä¿®å¤å»ºè®®å‡†ç¡®æ€§

3. **å¤š Agent åä½œ** (Day 11-12)
   - CodeReviewAgent: PR ä»£ç å®¡æŸ¥
   - MetricsAgent: DORA æŒ‡æ ‡åˆ†æ
   - ä½¿ç”¨ LangGraph ç¼–æ’å¤æ‚å·¥ä½œæµ

4. **æ€§èƒ½ä¼˜åŒ–** (Day 13-14)
   - æ‰¹é‡æ¨ç† (Batch Inference)
   - å“åº”ç¼“å­˜ (Redis)
   - å¼‚æ­¥å¹¶å‘å¤„ç†

---

## ğŸ“ æäº¤è¯´æ˜

æœ¬æ¬¡æäº¤å®Œæˆäº† **Week 2 Day 8** çš„æ‰€æœ‰ç›®æ ‡:

- âœ… å®ç° ModelServiceLLM è‡ªå®šä¹‰ LLM åŒ…è£…å™¨
- âœ… Agent Orchestrator é›†æˆæœ¬åœ° Model Service
- âœ… æ”¯æŒ OpenAI / æœ¬åœ°æ¨¡å‹çµæ´»åˆ‡æ¢
- âœ… å®Œæ•´çš„ç«¯åˆ°ç«¯æµ‹è¯•è„šæœ¬
- âœ… Docker Compose æœåŠ¡ä¾èµ–é…ç½®

**Week 1 è¿›åº¦**: 7/7 å¤©å®Œæˆ (100%)  
**Week 2 è¿›åº¦**: 1/7 å¤©å®Œæˆ (14%)

**ä¸‹ä¸€æ­¥**: Week 2 Day 9 - æµå¼å“åº” + çŸ¥è¯†åº“å·¥å…·é›†æˆ

---

## ğŸ”— ç›¸å…³èµ„æº

- [LangChain Custom LLM Guide](https://python.langchain.com/docs/modules/model_io/llms/custom_llm)
- [LangChain AgentExecutor](https://python.langchain.com/docs/modules/agents/agent_types/)
- [httpx Async Client](https://www.python-httpx.org/async/)
- [Docker Compose depends_on](https://docs.docker.com/compose/compose-file/05-services/#depends_on)

---

**æœ€åæ›´æ–°**: 2026-02-28  
**ä½œè€…**: Ren (AI Workflow é¡¹ç›®è´Ÿè´£äºº)
