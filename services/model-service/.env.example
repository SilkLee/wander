# Server Configuration
PORT=8004
DEBUG=true

# Model Configuration
MODEL_NAME=Qwen/Qwen2.5-7B-Instruct  # Or any HuggingFace model ID
MODEL_REVISION=main
DEVICE=cuda  # cuda or cpu
MAX_MODEL_LEN=4096

# Generation Parameters
DEFAULT_MAX_TOKENS=512
DEFAULT_TEMPERATURE=0.7
DEFAULT_TOP_P=0.9

# vLLM Configuration (if using vLLM)
USE_VLLM=false
VLLM_TENSOR_PARALLEL_SIZE=1
VLLM_GPU_MEMORY_UTILIZATION=0.9

# Cache Configuration
HF_HOME=/app/cache/huggingface
TRANSFORMERS_CACHE=/app/cache/transformers
