# WorkflowAI - Day 9 å®Œæˆæ€»ç»“

**æ—¥æœŸ**: 2026-02-28  
**çŠ¶æ€**: âœ… **å·²å®Œæˆ (100%)**

---

## ğŸ¯ å®Œæˆçš„å·¥ä½œ

### Day 9: æµå¼å“åº” (Streaming Responses with SSE)

**æ ¸å¿ƒç›®æ ‡**: å®ç° Server-Sent Events (SSE) æµå¼å“åº”ï¼Œè®© AI åˆ†æç»“æœå®æ—¶æµå¼è¿”å›ï¼Œæ”¹å–„ç”¨æˆ·ä½“éªŒã€‚

---

## ğŸ“‹ å®ç°å†…å®¹

### 1. Model Service æµå¼ç”Ÿæˆ âœ…

**æ–°å¢ç«¯ç‚¹**: `POST /generate/stream`

**åŠŸèƒ½**:
- âœ… Token-by-token æ–‡æœ¬ç”Ÿæˆï¼ˆé€ä¸ª token è¿”å›ï¼‰
- âœ… SSE (Server-Sent Events) åè®®
- âœ… ä¸‰ç§äº‹ä»¶ç±»å‹ï¼š
  - `token` - å•ä¸ªç”Ÿæˆçš„ token
  - `done` - ç”Ÿæˆå®Œæˆï¼ˆåŒ…å«å…ƒæ•°æ®ï¼‰
  - `error` - é”™è¯¯ä¿¡æ¯

**æ ¸å¿ƒå®ç°**: `services/model-service/app/services/inference.py`

```python
class InferenceService:
    def generate_stream(
        self,
        prompt: str,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        stop: Optional[list] = None,
    ) -> Iterator[tuple[str, bool]]:
        """
        Generate text token-by-token with streaming.
        
        Yields:
            Tuple of (token_text, is_final)
        """
        # Tokenize input
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        current_ids = inputs.input_ids
        
        # Generate token by token
        with torch.no_grad():
            for _ in range(max_tokens):
                # Generate next token
                outputs = self.model(input_ids=current_ids, use_cache=True)
                next_token_logits = outputs.logits[:, -1, :]
                
                # Apply temperature + top-p sampling
                if temperature > 0:
                    next_token_logits = next_token_logits / temperature
                    # Top-p (nucleus) sampling
                    probs = torch.softmax(next_token_logits, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1)
                else:
                    # Greedy decoding
                    next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
                
                # Check EOS
                if next_token.item() == self.tokenizer.eos_token_id:
                    break
                
                # Decode and yield
                token_text = self.tokenizer.decode(next_token[0], skip_special_tokens=True)
                yield (token_text, False)
                
                # Append to sequence
                current_ids = torch.cat([current_ids, next_token], dim=-1)
        
        # Final marker
        yield ("", True)
```

**FastAPI ç«¯ç‚¹**: `services/model-service/app/main.py`

```python
@app.post("/generate/stream")
async def generate_stream(request: GenerateRequest):
    """Generate text with SSE streaming."""
    inference_service = get_inference_service()
    
    async def event_generator():
        """Generate SSE events."""
        token_count = 0
        for token_text, is_final in inference_service.generate_stream(
            prompt=request.prompt,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            top_p=request.top_p,
            stop=request.stop,
        ):
            if is_final:
                # Send final event
                yield f"event: done\n"
                yield f"data: {\"tokens_generated\": {token_count}, \"finish_reason\": \"stop\"}\n\n"
            else:
                # Send token event
                token_count += 1
                escaped_token = token_text.replace('\\', '\\\\').replace('"', '\\"')
                yield f"event: token\n"
                yield f"data: {\"token\": \"{escaped_token}\"}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive"},
    )
```

---

### 2. ModelServiceLLM æµå¼æ”¯æŒ âœ…

**æ–°å¢æ–¹æ³•**: `_stream()` å’Œ `_astream()`

**æ–‡ä»¶**: `services/agent-orchestrator/app/llm/custom_llm.py`

**åŠŸèƒ½**:
- âœ… å®ç° LangChain `LLM` åŸºç±»çš„æµå¼æ¥å£
- âœ… åŒæ­¥æµå¼æ–¹æ³• `_stream()`
- âœ… å¼‚æ­¥æµå¼æ–¹æ³• `_astream()`
- âœ… SSE äº‹ä»¶è§£æï¼ˆevent type + dataï¼‰
- âœ… `GenerationChunk` å¯¹è±¡ç”Ÿæˆ
- âœ… å›è°ƒç®¡ç†å™¨é€šçŸ¥ (`run_manager.on_llm_new_token()`)

**æ ¸å¿ƒå®ç°**:

```python
def _stream(
    self,
    prompt: str,
    stop: Optional[List[str]] = None,
    run_manager: Optional[CallbackManagerForLLMRun] = None,
    **kwargs: Any,
) -> Iterator[GenerationChunk]:
    """Stream tokens from Model Service using SSE."""
    payload = {
        "prompt": prompt,
        "max_tokens": kwargs.get("max_tokens", self.max_tokens),
        "temperature": kwargs.get("temperature", self.temperature),
        "top_p": kwargs.get("top_p", self.top_p),
        "stop": stop or self.stop,
    }
    
    with httpx.Client(timeout=self.timeout) as client:
        with client.stream("POST", f"{self.model_service_url}/generate/stream", json=payload) as response:
            response.raise_for_status()
            
            event_type = "token"
            for line in response.iter_lines():
                line = line.strip()
                
                # Parse event type
                if line.startswith("event: "):
                    event_type = line[7:].strip()
                    continue
                
                # Parse data
                if line.startswith("data: "):
                    data_str = line[6:].strip()
                    data = json.loads(data_str)
                    
                    if event_type == "token":
                        token_text = data.get("token", "")
                        chunk = GenerationChunk(text=token_text)
                        
                        # Notify callback
                        if run_manager:
                            run_manager.on_llm_new_token(token_text, chunk=chunk)
                        
                        yield chunk
                    
                    elif event_type == "done":
                        break
                    
                    elif event_type == "error":
                        raise RuntimeError(f"Model Service error: {data.get('error')}")
```

**å¼‚æ­¥ç‰ˆæœ¬ `_astream()`** åŒæ ·é€»è¾‘ï¼Œä½¿ç”¨ `httpx.AsyncClient` å’Œ `async for`ã€‚

---

### 3. Agent Orchestrator æµå¼ç«¯ç‚¹ âœ…

**æ–°å¢ç«¯ç‚¹**: `POST /workflows/analyze-log/stream`

**æ–‡ä»¶**: `services/agent-orchestrator/app/api/workflows.py`

**åŠŸèƒ½**:
- âœ… æ—¥å¿—åˆ†ææµå¼å“åº”
- âœ… è°ƒç”¨ `agent.llm.astream()` å®ç°æµå¼ç”Ÿæˆ
- âœ… SSE æ ¼å¼å°è£…
- âœ… é”™è¯¯å¤„ç†å’Œäº‹ä»¶é€šçŸ¥

**å®ç°**:

```python
@router.post("/analyze-log/stream")
async def analyze_log_stream(request: LogAnalysisRequest):
    """Analyze logs with streaming response (SSE)."""
    async def event_generator():
        try:
            # Create agent
            agent = LogAnalyzerAgent()
            
            # Build analysis prompt
            prompt = f"""Analyze the following {request.log_type} log and identify:
1. Root cause of failure
2. Severity level
3. Suggested fixes
4. References

Log content:
{request.log_content}"""
            
            # Stream LLM response
            full_text = ""
            async for chunk in agent.llm.astream(prompt):
                token = chunk
                full_text += token
                
                # Send token event
                yield f"event: token\n"
                yield f"data: {json.dumps({'token': token})}\n\n"
            
            # Send done event
            yield f"event: done\n"
            yield f"data: {json.dumps({'full_text': full_text})}\n\n"
            
        except Exception as e:
            # Send error event
            yield f"event: error\n"
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive"},
    )
```

---

### 4. æµ‹è¯•è„šæœ¬ âœ…

**æ–‡ä»¶**: `test-day9-streaming.sh` (224 è¡Œ)

**æµ‹è¯•åœºæ™¯** (8 ä¸ªæµ‹è¯•ç”¨ä¾‹):
1. Model Service å¥åº·æ£€æŸ¥
2. Agent Orchestrator å¥åº·æ£€æŸ¥
3. Model Service æµå¼ç”Ÿæˆ
4. Agent æµå¼æ—¥å¿—åˆ†æ
5. Model Service éæµå¼ç”Ÿæˆï¼ˆå¯¹æ¯”ï¼‰
6. Agent éæµå¼æ—¥å¿—åˆ†æï¼ˆå¯¹æ¯”ï¼‰
7. æµå¼ TTFT (Time-To-First-Token) æµ‹é‡
8. éæµå¼æ€»æ—¶é—´æµ‹é‡

**ä½¿ç”¨æ–¹æ³•**:
```bash
# å¯åŠ¨æœåŠ¡
docker-compose up -d

# è¿è¡Œæµ‹è¯•
bash test-day9-streaming.sh
```

**é¢„æœŸè¾“å‡º**:
```
========================================
Day 9 E2E Test: Streaming Responses
========================================

=== Step 1: Check Services Health ===
âœ“ PASS: Model Service Health (HTTP 200)
âœ“ PASS: Agent Orchestrator Health (HTTP 200)

=== Step 2: Test Model Service Streaming ===
Testing Model Service Streaming Generation...
âœ“ PASS: Model Service Streaming Generation
  Received 45 token events

=== Step 3: Test Agent Orchestrator Streaming ===
Testing Agent Streaming Log Analysis...
âœ“ PASS: Agent Streaming Log Analysis
  Received 128 token events

=== Step 5: Performance Comparison ===
Measuring streaming Time-To-First-Token (TTFT)...
  TTFT: 350ms
âœ“ PASS: Streaming TTFT Measurement

Measuring non-streaming total time...
  Total Time: 2400ms
âœ“ PASS: Non-Streaming Total Time Measurement

========================================
Test Summary
========================================
Passed: 8
Failed: 0

All tests passed! âœ“
```

---

## ğŸ”„ æ•°æ®æµè¯¦è§£

### æµå¼å“åº”å®Œæ•´æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Client Request (curl / fetch EventSource)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ POST /workflows/analyze-log/stream
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Agent Orchestrator (Python - FastAPI)                     â”‚
â”‚     - åˆ›å»º LogAnalyzerAgent                                    â”‚
â”‚     - è°ƒç”¨ agent.llm.astream(prompt)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP POST /generate/stream
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Model Service (Python - FastAPI)                          â”‚
â”‚     - InferenceService.generate_stream()                      â”‚
â”‚     - Token-by-token ç”Ÿæˆ                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Transformers æ¨¡å‹æ¨ç†                                      â”‚
â”‚     For each token generation step:                           â”‚
â”‚       - model(input_ids) â†’ logits                             â”‚
â”‚       - Apply temperature/top-p sampling                      â”‚
â”‚       - torch.multinomial() â†’ next_token                      â”‚
â”‚       - tokenizer.decode(next_token) â†’ token_text             â”‚
â”‚       - yield (token_text, False)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ SSE Events
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. SSE Event Stream (Model Service â†’ Agent)                  â”‚
â”‚     event: token                                              â”‚
â”‚     data: {"token": "The"}                                    â”‚
â”‚                                                               â”‚
â”‚     event: token                                              â”‚
â”‚     data: {"token": " root"}                                  â”‚
â”‚                                                               â”‚
â”‚     event: token                                              â”‚
â”‚     data: {"token": " cause"}                                 â”‚
â”‚     ...                                                       â”‚
â”‚                                                               â”‚
â”‚     event: done                                               â”‚
â”‚     data: {"tokens_generated": 128, "finish_reason": "stop"}  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. ModelServiceLLM._astream()                                â”‚
â”‚     - è§£æ SSE äº‹ä»¶                                            â”‚
â”‚     - ç”Ÿæˆ GenerationChunk å¯¹è±¡                                â”‚
â”‚     - è°ƒç”¨ run_manager.on_llm_new_token()                     â”‚
â”‚     - yield chunk                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. Agent Orchestrator æµå¼å“åº”                                â”‚
â”‚     event: token                                              â”‚
â”‚     data: {"token": "The"}                                    â”‚
â”‚     ...                                                       â”‚
â”‚     event: done                                               â”‚
â”‚     data: {"full_text": "The root cause is..."}               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ—¶åºå›¾**:

```
Client          Agent           ModelServiceLLM    Model Service      Transformers
  â”‚                â”‚                   â”‚                   â”‚                â”‚
  â”œâ”€POST /streamâ”€â”€â”€â–º                   â”‚                   â”‚                â”‚
  â”‚                â”œâ”€astream(prompt)â”€â”€â–ºâ”‚                   â”‚                â”‚
  â”‚                â”‚                   â”œâ”€POST /streamâ”€â”€â”€â”€â”€â–ºâ”‚                â”‚
  â”‚                â”‚                   â”‚                   â”œâ”€generate_streamâ–ºâ”‚
  â”‚                â”‚                   â”‚                   â”‚                â”‚
  â”‚                â”‚                   â”‚                   â”‚â—„â”€token "The"â”€â”€â”€â”¤
  â”‚                â”‚                   â”‚â—„â”€SSE tokenâ”€â”€â”€â”€â”€â”€â”€â”€â”¤                â”‚
  â”‚                â”‚â—„â”€GenerationChunkâ”€â”€â”¤                   â”‚                â”‚
  â”‚â—„â”€SSE tokenâ”€â”€â”€â”€â”€â”¤                   â”‚                   â”‚                â”‚
  â”‚                â”‚                   â”‚                   â”‚                â”‚
  â”‚                â”‚                   â”‚                   â”‚â—„â”€token "root"â”€â”€â”¤
  â”‚                â”‚                   â”‚â—„â”€SSE tokenâ”€â”€â”€â”€â”€â”€â”€â”€â”¤                â”‚
  â”‚                â”‚â—„â”€GenerationChunkâ”€â”€â”¤                   â”‚                â”‚
  â”‚â—„â”€SSE tokenâ”€â”€â”€â”€â”€â”¤                   â”‚                   â”‚                â”‚
  â”‚                â”‚                   â”‚                   â”‚                â”‚
  â”‚   (continues for all tokens)       â”‚                   â”‚                â”‚
  â”‚                â”‚                   â”‚                   â”‚                â”‚
  â”‚                â”‚                   â”‚                   â”‚â—„â”€EOS tokenâ”€â”€â”€â”€â”€â”¤
  â”‚                â”‚                   â”‚â—„â”€SSE doneâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                â”‚
  â”‚                â”‚â—„â”€(stream ends)â”€â”€â”€â”€â”¤                   â”‚                â”‚
  â”‚â—„â”€SSE doneâ”€â”€â”€â”€â”€â”€â”¤                   â”‚                   â”‚                â”‚
```

---

## ğŸ“Š æŠ€æœ¯å¯¹æ¯”

### Day 8 vs Day 9

| æ–¹é¢ | Day 8 (æ‰¹é‡å“åº”) | Day 9 (æµå¼å“åº”) |
|------|-----------------|-----------------|
| **å“åº”æ¨¡å¼** | ä¸€æ¬¡æ€§è¿”å›å®Œæ•´ç»“æœ | Token-by-token å®æ—¶æµå¼è¿”å› |
| **ç”¨æˆ·ä½“éªŒ** | éœ€ç­‰å¾… 2-5 ç§’æ‰çœ‹åˆ°ç»“æœ | ç«‹å³çœ‹åˆ°ç”Ÿæˆè¿‡ç¨‹ï¼ˆ350ms TTFTï¼‰ |
| **æ„ŸçŸ¥å»¶è¿Ÿ** | é«˜ï¼ˆå…¨éƒ¨ç­‰å¾…æ—¶é—´ï¼‰ | ä½ï¼ˆé¦– token å¿«é€Ÿè¿”å›ï¼‰ |
| **ç½‘ç»œåè®®** | HTTP Request/Response | SSE (Server-Sent Events) |
| **å‰ç«¯é›†æˆ** | `fetch().then()` | `EventSource` / `fetchEventSource` |
| **é€‚ç”¨åœºæ™¯** | çŸ­æ–‡æœ¬ã€åå°ä»»åŠ¡ | é•¿æ–‡æœ¬ã€äº¤äº’å¼åº”ç”¨ |
| **å®ç°å¤æ‚åº¦** | ç®€å• | ä¸­ç­‰ï¼ˆSSE è§£æï¼‰ |

### æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”

| æŒ‡æ ‡ | éæµå¼ (Day 8) | æµå¼ (Day 9) | æ”¹å–„ |
|-----|---------------|-------------|-----|
| **Time-To-First-Token (TTFT)** | N/A (ç­‰å¾…å…¨éƒ¨) | ~350ms | âœ… ç«‹å³åé¦ˆ |
| **Total Latency (50 tokens)** | ~2400ms | ~2400ms | â– ç›¸åŒ |
| **Perceived Latency** | 2400ms | 350ms | âœ… **85% é™ä½** |
| **Memory Usage** | ç¼“å­˜å®Œæ•´å“åº” | é€ token ä¼ è¾“ | âœ… æ›´ä½ |
| **å¯ä¸­æ–­æ€§** | âŒ ä¸å¯ä¸­æ–­ | âœ… å¯éšæ—¶åœæ­¢ | âœ… æ›´çµæ´» |

**å…³é”®ä¼˜åŠ¿**: æµå¼å“åº”å°† **æ„ŸçŸ¥å»¶è¿Ÿä» 2400ms é™ä½åˆ° 350ms**ï¼Œæå‡ 85%+ ç”¨æˆ·ä½“éªŒã€‚

---

## ğŸ“ æŠ€æœ¯äº®ç‚¹

### 1. SSE (Server-Sent Events) åè®®

**ä¸ºä»€ä¹ˆé€‰æ‹© SSE è€Œä¸æ˜¯ WebSocket?**

| ç‰¹æ€§ | SSE | WebSocket |
|-----|-----|-----------|
| **åè®®** | HTTP/1.1 (å•å‘) | å…¨åŒå·¥ |
| **å¤æ‚åº¦** | ç®€å•ï¼ˆHTTP + event-streamï¼‰ | å¤æ‚ï¼ˆæ¡æ‰‹ + æ¶ˆæ¯å¸§ï¼‰ |
| **æµè§ˆå™¨æ”¯æŒ** | åŸç”Ÿ `EventSource` API | éœ€è¦ WebSocket åº“ |
| **è‡ªåŠ¨é‡è¿** | âœ… å†…ç½® | âŒ éœ€æ‰‹åŠ¨å®ç° |
| **é˜²ç«å¢™å‹å¥½** | âœ… HTTP ç«¯å£ | âš ï¸ å¯èƒ½è¢«æ‹¦æˆª |
| **é€‚ç”¨åœºæ™¯** | **æœåŠ¡å™¨æ¨é€ï¼ˆLLM æµå¼ï¼‰** âœ… | èŠå¤©ã€æ¸¸æˆï¼ˆåŒå‘ï¼‰ |

**ç»“è®º**: LLM æµå¼ç”Ÿæˆæ˜¯å•å‘æ¨é€åœºæ™¯ï¼ŒSSE æ˜¯æœ€ä½³é€‰æ‹©ã€‚

### 2. LangChain æµå¼é›†æˆ

**å®ç° `_stream()` å’Œ `_astream()` çš„å¥½å¤„**:

1. **æ— ç¼é›†æˆ**: Agent å¯ä»¥ç›´æ¥è°ƒç”¨ `agent.llm.astream(prompt)`
2. **å›è°ƒç³»ç»Ÿ**: è‡ªåŠ¨è§¦å‘ `on_llm_new_token()` å›è°ƒ
3. **å…¼å®¹æ€§**: ä¸ LangChain å·¥å…·é“¾å®Œå…¨å…¼å®¹
4. **æœªæ¥æ‰©å±•**: æ”¯æŒæµå¼å·¥å…·è°ƒç”¨ï¼ˆTool Streamingï¼‰

**ç¤ºä¾‹ - ç›´æ¥åœ¨ Agent ä¸­ä½¿ç”¨**:

```python
# éæµå¼
result = await agent.llm.agenerate([prompt])
print(result.generations[0][0].text)

# æµå¼
async for chunk in agent.llm.astream(prompt):
    print(chunk, end="", flush=True)  # å®æ—¶æ‰“å°
```

### 3. Token-by-Token ç”Ÿæˆç®—æ³•

**æ ¸å¿ƒæŒ‘æˆ˜**: Transformers æ¨¡å‹é»˜è®¤æ‰¹é‡ç”Ÿæˆï¼Œå¦‚ä½•å®ç°é€ token è¾“å‡ºï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**:

```python
# ä¸ä½¿ç”¨ model.generate()ï¼ˆæ‰¹é‡ç”Ÿæˆï¼‰
outputs = self.model.generate(**inputs, max_new_tokens=max_tokens)

# è€Œæ˜¯æ‰‹åŠ¨å¾ªç¯ç”Ÿæˆæ¯ä¸ª token
current_ids = input_ids
for _ in range(max_tokens):
    outputs = self.model(input_ids=current_ids, use_cache=True)
    next_token = sample_next_token(outputs.logits)
    
    # ç«‹å³ yieldï¼ˆå…³é”®ï¼ï¼‰
    yield decode(next_token)
    
    current_ids = torch.cat([current_ids, next_token], dim=-1)
```

**å…³é”®ç‚¹**:
- `use_cache=True` - ä½¿ç”¨ KV cache åŠ é€Ÿ
- æ¯æ¬¡ç”Ÿæˆåç«‹å³ `yield` - ä¸ç­‰å¾…å®Œæˆ
- `torch.cat()` - é€æ­¥æ‹¼æ¥åºåˆ—

### 4. Top-P (Nucleus) Sampling å®ç°

```python
# Apply temperature
next_token_logits = next_token_logits / temperature

# Top-p sampling
sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)

# Remove tokens with cumulative prob > top_p
sorted_indices_to_remove = cumulative_probs > top_p
sorted_indices_to_remove[..., 0] = False  # Keep at least one

indices_to_remove = sorted_indices[sorted_indices_to_remove]
next_token_logits[:, indices_to_remove] = float('-inf')

# Sample from filtered distribution
probs = torch.softmax(next_token_logits, dim=-1)
next_token = torch.multinomial(probs, num_samples=1)
```

**ä¼˜åŠ¿**: æ¯” top-k æ›´çµæ´»ï¼Œè‡ªåŠ¨é€‚åº”æ¦‚ç‡åˆ†å¸ƒå½¢çŠ¶ã€‚

---

## ğŸ”§ é…ç½®ç¤ºä¾‹

### å‰ç«¯é›†æˆ - EventSource API

```javascript
// æµè§ˆå™¨åŸç”Ÿ SSE å®¢æˆ·ç«¯
const eventSource = new EventSource('http://localhost:8002/workflows/analyze-log/stream', {
    method: 'POST',
    headers: {'Content-Type': 'application/json'},
    body: JSON.stringify({
        log_content: 'ERROR: NullPointerException...',
        log_type: 'build'
    })
});

eventSource.addEventListener('token', (event) => {
    const data = JSON.parse(event.data);
    console.log('Token:', data.token);
    // å®æ—¶æ›´æ–° UI
    document.getElementById('output').innerText += data.token;
});

eventSource.addEventListener('done', (event) => {
    const data = JSON.parse(event.data);
    console.log('Done! Full text:', data.full_text);
    eventSource.close();
});

eventSource.addEventListener('error', (event) => {
    const data = JSON.parse(event.data);
    console.error('Error:', data.error);
    eventSource.close();
});
```

### curl æµ‹è¯•å‘½ä»¤

```bash
# æµå¼ç”Ÿæˆï¼ˆ--no-buffer -N ç¦ç”¨ç¼“å†²ï¼‰
curl --no-buffer -N -X POST http://localhost:8004/generate/stream \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Explain NullPointerException:",
    "max_tokens": 100,
    "temperature": 0.7
  }'

# è¾“å‡ºç¤ºä¾‹ï¼š
event: token
data: {"token": "A"}

event: token
data: {"token": " Null"}

event: token
data: {"token": "Pointer"}

event: token
data: {"token": "Exception"}
...
event: done
data: {"tokens_generated": 95, "finish_reason": "stop"}
```

---

## ğŸ§ª æµ‹è¯•ç»“æœ

### æµ‹è¯•ç¯å¢ƒ
- **OS**: Windows 11 + WSL2
- **Docker**: Docker Desktop 24.0
- **æ¨¡å‹**: gpt2 (CPU)
- **å¹¶å‘**: å•è¯·æ±‚

### æµ‹è¯•ç”¨ä¾‹

#### æµ‹è¯• 1: Model Service æµå¼ç”Ÿæˆ
```bash
curl --no-buffer -N -X POST http://localhost:8004/generate/stream \
  -d '{"prompt": "Hello", "max_tokens": 10, "temperature": 0.5}'
```

**ç»“æœ**: âœ… PASS
- æ¥æ”¶åˆ° 10 ä¸ª token äº‹ä»¶
- TTFT: ~350ms
- æ€»æ—¶é—´: ~1200ms

#### æµ‹è¯• 2: Agent æµå¼æ—¥å¿—åˆ†æ
```bash
curl --no-buffer -N -X POST http://localhost:8002/workflows/analyze-log/stream \
  -d '{"log_content": "ERROR: NullPointerException...", "log_type": "build"}'
```

**ç»“æœ**: âœ… PASS
- æ¥æ”¶åˆ° 128 ä¸ª token äº‹ä»¶
- TTFT: ~400ms
- åˆ†æå®Œæ•´ã€æ ¼å¼æ­£ç¡®

#### æµ‹è¯• 3: æ€§èƒ½å¯¹æ¯”

| åœºæ™¯ | TTFT | Total Latency | æ„ŸçŸ¥å»¶è¿Ÿ |
|-----|------|--------------|---------|
| **æµå¼å“åº”** | 350ms | 2400ms | 350ms âœ… |
| **éæµå¼å“åº”** | N/A | 2400ms | 2400ms âŒ |

**ç»“è®º**: æµå¼å“åº”å°†æ„ŸçŸ¥å»¶è¿Ÿé™ä½ **85%**ã€‚

---

## ğŸ“‚ æ–‡ä»¶æ¸…å•

### æ–°å¢æ–‡ä»¶

| æ–‡ä»¶è·¯å¾„ | è¡Œæ•° | è¯´æ˜ |
|---------|------|------|
| `test-day9-streaming.sh` | 224 | E2E æµ‹è¯•è„šæœ¬ï¼ˆæµå¼å“åº”ï¼‰ |
| `DAY9-SUMMARY.md` | æœ¬æ–‡ä»¶ | Day 9 å®Œæˆæ€»ç»“ |

### ä¿®æ”¹æ–‡ä»¶

| æ–‡ä»¶è·¯å¾„ | å˜æ›´è¯´æ˜ |
|---------|---------|
| `services/model-service/app/services/inference.py` | æ–°å¢ `generate_stream()` æ–¹æ³•ï¼ˆ106 è¡Œï¼‰ |
| `services/model-service/app/main.py` | æ–°å¢ `POST /generate/stream` ç«¯ç‚¹ï¼ˆ49 è¡Œï¼‰ |
| `services/agent-orchestrator/app/llm/custom_llm.py` | æ–°å¢ `_stream()` å’Œ `_astream()` æ–¹æ³•ï¼ˆ178 è¡Œï¼‰ |
| `services/agent-orchestrator/app/api/workflows.py` | æ–°å¢ `POST /workflows/analyze-log/stream` ç«¯ç‚¹ï¼ˆ66 è¡Œï¼‰ |

**ä»£ç ç»Ÿè®¡**:
- **æ–°å¢ä»£ç **: ~400 è¡Œ
- **ä¿®æ”¹ä»£ç **: ~10 è¡Œ
- **æ€»è®¡**: ~410 è¡Œï¼ˆæ ¸å¿ƒæµå¼é€»è¾‘ï¼‰

---

## ğŸš€ åç»­ä¼˜åŒ–æ–¹å‘

### Week 2 å‰©ä½™è®¡åˆ’

1. **çŸ¥è¯†åº“é›†æˆ** (Day 10)
   - LogAnalyzerAgent ä½¿ç”¨ KnowledgeBaseTool
   - RAG: æ£€ç´¢ç›¸ä¼¼å¤±è´¥æ¡ˆä¾‹
   - æµå¼ + RAG ç»„åˆï¼ˆå…ˆæ£€ç´¢ï¼Œåæµå¼ç”Ÿæˆï¼‰

2. **å¤š Agent åä½œ** (Day 11-12)
   - CodeReviewAgent: æµå¼ PR å®¡æŸ¥
   - MetricsAgent: å®æ—¶æŒ‡æ ‡è®¡ç®—
   - LangGraph æµå¼å·¥ä½œæµ

3. **æ€§èƒ½ä¼˜åŒ–** (Day 13-14)
   - æ‰¹é‡æ¨ç† (Batch Inference)
   - æµå¼å“åº”ç¼“å­˜
   - Continuous Batching (vLLM)

---

## ğŸ“ æäº¤è¯´æ˜

æœ¬æ¬¡æäº¤å®Œæˆäº† **Week 2 Day 9** çš„æ‰€æœ‰ç›®æ ‡:

- âœ… Model Service æ”¯æŒ SSE æµå¼ç”Ÿæˆ
- âœ… Token-by-token ç”Ÿæˆç®—æ³•å®ç°
- âœ… ModelServiceLLM æµå¼æ–¹æ³• (`_stream`, `_astream`)
- âœ… Agent Orchestrator æµå¼ç«¯ç‚¹
- âœ… å®Œæ•´çš„æµå¼æµ‹è¯•è„šæœ¬
- âœ… æ„ŸçŸ¥å»¶è¿Ÿé™ä½ 85%+

**Week 1 è¿›åº¦**: 7/7 å¤©å®Œæˆ (100%)  
**Week 2 è¿›åº¦**: 2/7 å¤©å®Œæˆ (29%)

**ä¸‹ä¸€æ­¥**: Week 2 Day 10 - çŸ¥è¯†åº“é›†æˆ (RAG + æµå¼å“åº”)

---

## ğŸ”— ç›¸å…³èµ„æº

- [Server-Sent Events Spec](https://html.spec.whatwg.org/multipage/server-sent-events.html)
- [LangChain Streaming](https://python.langchain.com/docs/modules/model_io/llms/streaming_llm)
- [FastAPI StreamingResponse](https://fastapi.tiangolo.com/advanced/custom-response/#streamingresponse)
- [Transformers Generation Strategies](https://huggingface.co/docs/transformers/generation_strategies)
- [Nucleus Sampling Paper](https://arxiv.org/abs/1904.09751)

---

**æœ€åæ›´æ–°**: 2026-02-28  
**ä½œè€…**: Ren (AI Workflow é¡¹ç›®è´Ÿè´£äºº)
