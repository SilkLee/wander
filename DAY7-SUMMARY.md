# WorkflowAI - Day 7 å®Œæˆæ€»ç»“

**æ—¥æœŸ**: 2026-02-27  
**çŠ¶æ€**: âœ… **å·²å®Œæˆ (100%)**

---

## ğŸ¯ å®Œæˆçš„å·¥ä½œ

### 1. Model Service å®ç° (ç«¯å£ 8004)

**ä»£ç é‡**: 487 è¡Œ Python ä»£ç ï¼Œæ–°å¢/ä¿®æ”¹ 6 ä¸ªæ–‡ä»¶

**æ ¸å¿ƒåŠŸèƒ½**:
- âœ… LLM æ–‡æœ¬ç”Ÿæˆ (Transformers åº“)
- âœ… æœ¬åœ°æ¨¡å‹åŠ è½½æ”¯æŒ (ç¦»çº¿è¿è¡Œ)
- âœ… è‡ªåŠ¨è®¾å¤‡æ£€æµ‹ (CUDA/CPU)
- âœ… å¯é…ç½®ç”Ÿæˆå‚æ•° (temperature, top_p, max_tokens)
- âœ… å¥åº·æ£€æŸ¥å’Œæ¨¡å‹ä¿¡æ¯ç«¯ç‚¹
- âœ… Lazy loading æ¨¡å¼ (é¦–æ¬¡è¯·æ±‚åŠ è½½æ¨¡å‹)

**API ç«¯ç‚¹**:
```
GET  /             - æœåŠ¡ä¿¡æ¯
GET  /health       - å¥åº·æ£€æŸ¥ (åŒ…å«æ¨¡å‹åŠ è½½çŠ¶æ€)
GET  /ready        - Kubernetes å°±ç»ªæ¢é’ˆ
GET  /live         - Kubernetes å­˜æ´»æ¢é’ˆ
POST /generate     - æ–‡æœ¬ç”Ÿæˆ
GET  /model/info   - æ¨¡å‹ä¿¡æ¯
```

**æŠ€æœ¯ç‰¹æ€§**:
- **æœ¬åœ°æ¨¡å‹æ”¯æŒ**: é€šè¿‡ `LOCAL_MODEL_PATH` ç¯å¢ƒå˜é‡æ”¯æŒæœ¬åœ°æ¨¡å‹ç›®å½•
- **ç¦»çº¿è¿è¡Œ**: `local_files_only=True` é¿å…ç½‘ç»œè®¿é—®
- **å†…å­˜ä¼˜åŒ–**: `low_cpu_mem_usage=True` å‡å°‘åŠ è½½æ—¶å†…å­˜å ç”¨
- **è®¾å¤‡å›é€€**: CUDA ä¸å¯ç”¨æ—¶è‡ªåŠ¨é™çº§åˆ° CPU
- **çµæ´»é…ç½®**: æ”¯æŒä»»ä½• HuggingFace å…¼å®¹æ¨¡å‹

---

## ğŸ“ æ–‡ä»¶æ¸…å•

### æ ¸å¿ƒæœåŠ¡ä»£ç 

#### ä¿®æ”¹çš„æ–‡ä»¶

**services/model-service/app/config.py** (85 è¡Œ)
```python
# æ–°å¢é…ç½®é¡¹
local_model_path: Optional[str] = Field(
    default=None,
    description="Local model directory path (overrides model_name)",
)
```

**services/model-service/app/services/inference.py** (148 è¡Œ)
```python
class InferenceService:
    def __init__(self):
        # æ”¯æŒæœ¬åœ°æ¨¡å‹å’Œ HuggingFace æ¨¡å‹
        self.model_path = settings.local_model_path or settings.model_name
        self.is_local = settings.local_model_path is not None
        
        # åŠ è½½ tokenizer å’Œæ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            local_files_only=self.is_local,  # ç¦»çº¿æ¨¡å¼
            trust_remote_code=True,
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=torch.float16 if cuda else torch.float32,
            local_files_only=self.is_local,
            low_cpu_mem_usage=True,  # å†…å­˜ä¼˜åŒ–
        )
    
    def generate(self, prompt, max_tokens, temperature, top_p, stop):
        # æ–‡æœ¬ç”Ÿæˆé€»è¾‘
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=temperature > 0,
        )
        return generated_text, tokens_generated, finish_reason
    
    def get_model_info(self) -> dict:
        # è¿”å›æ¨¡å‹è¯¦ç»†ä¿¡æ¯
        return {
            "name": self.model_name,
            "path": self.model_path,
            "is_local": self.is_local,
            "type": "transformers",
            "device": self.device,
            ...
        }
```

**services/model-service/.env.example** (24 è¡Œ)
```bash
# æ–°å¢æœ¬åœ°æ¨¡å‹è·¯å¾„é…ç½®
LOCAL_MODEL_PATH=  # Optional: /app/models/Qwen2.5-7B-Instruct
```

**docker-compose.yml** (ä¿®æ”¹)
```yaml
model-service:
  environment:
    - MODEL_NAME=gpt2  # é»˜è®¤ä½¿ç”¨è½»é‡çº§æ¨¡å‹æµ‹è¯•
    - LOCAL_MODEL_PATH=/app/models/qwen  # å¯é€‰ï¼šæœ¬åœ°æ¨¡å‹è·¯å¾„
  volumes:
    - model_cache:/app/cache
    # å–æ¶ˆæ³¨é‡Šä»¥æŒ‚è½½æœ¬åœ°æ¨¡å‹ï¼ˆéœ€å…ˆä¸‹è½½ï¼‰
    # - C:/develop/Qwen2.5-7B-Instruct:/app/models/qwen:ro
```

#### API Gateway é›†æˆ

**services/api-gateway/main.go** (æ–°å¢è·¯ç”±)
```go
// Model Service - LLM inference
api.POST("/generate", utils.ProxyToService(cfg.ModelServiceURL))
api.GET("/model/info", utils.ProxyToService(cfg.ModelServiceURL+"/model/info"))
```

**è¯´æ˜**: `ModelServiceURL` é…ç½®åœ¨ Day 4 å·²æ·»åŠ ï¼Œæ— éœ€ä¿®æ”¹ `config.go`

---

## ğŸ§ª æµ‹è¯•è„šæœ¬

### 1. test-model-build.ps1 (183 è¡Œ)
**ç”¨é€”**: PowerShell è‡ªåŠ¨åŒ–æ„å»ºã€å¯åŠ¨å’Œæµ‹è¯•è„šæœ¬

**åŠŸèƒ½**:
- æ„å»º Docker é•œåƒ
- å¯åŠ¨å®¹å™¨å¹¶ç­‰å¾…æ¨¡å‹åŠ è½½ï¼ˆæœ€å¤š 5 åˆ†é’Ÿï¼‰
- è‡ªåŠ¨è¿è¡Œå¥åº·æ£€æŸ¥å’Œæ–‡æœ¬ç”Ÿæˆæµ‹è¯•
- å½©è‰²è¾“å‡ºï¼Œæ¸…æ™°çš„è¿›åº¦æç¤º

**ä½¿ç”¨æ–¹æ³•**:
```powershell
cd C:\develop\workflow-ai
.\test-model-build.ps1
```

### 2. build-model-service.sh (76 è¡Œ)
**ç”¨é€”**: Bash ç‰ˆæœ¬çš„æ„å»ºè„šæœ¬ï¼ˆGit Bash ç”¨æˆ·ï¼‰

**ä½¿ç”¨æ–¹æ³•**:
```bash
bash build-model-service.sh
```

### 3. test-model-e2e.sh (218 è¡Œ)
**ç”¨é€”**: å®Œæ•´çš„ç«¯åˆ°ç«¯æµ‹è¯•å¥—ä»¶

**æµ‹è¯•åœºæ™¯** (10 ä¸ªæµ‹è¯•ç”¨ä¾‹):
1. Root endpoint (æœåŠ¡ä¿¡æ¯)
2. Liveness probe (å­˜æ´»æ£€æŸ¥)
3. Readiness probe (å°±ç»ªæ£€æŸ¥)
4. Health check (å¥åº·çŠ¶æ€ + æ¨¡å‹åŠ è½½)
5. Model info (æ¨¡å‹è¯¦ç»†ä¿¡æ¯)
6. Simple text generation (ç®€å•ç”Ÿæˆ)
7. Coding prompt (ä»£ç ç”Ÿæˆ)
8. Deterministic generation (temperature=0)
9. Stop sequence (åœæ­¢åºåˆ—)
10. Error handling (æ— æ•ˆè¯·æ±‚)

**ä½¿ç”¨æ–¹æ³•**:
```bash
bash test-model-e2e.sh
```

### 4. test-model-local.sh (138 è¡Œ)
**ç”¨é€”**: æœ¬åœ° Python ç¯å¢ƒæµ‹è¯•ï¼ˆä¸ä¾èµ– Dockerï¼‰

**ä½¿ç”¨åœºæ™¯**: 
- å¿«é€ŸéªŒè¯æœ¬åœ°ä¸‹è½½çš„æ¨¡å‹
- å¼€å‘è°ƒè¯•æ—¶æµ‹è¯•ä»£ç æ›´æ”¹

**ä½¿ç”¨æ–¹æ³•**:
```bash
bash test-model-local.sh
```

---

## ğŸ”§ æŠ€æœ¯å®ç°ç»†èŠ‚

### æ¨¡å‹åŠ è½½ç­–ç•¥

#### æ–¹æ¡ˆ A: HuggingFace è‡ªåŠ¨ä¸‹è½½ (é»˜è®¤ - gpt2)
```yaml
environment:
  - MODEL_NAME=gpt2  # ~500MB, å¿«é€Ÿå¯åŠ¨
  - DEVICE=cpu
```

**ä¼˜ç‚¹**:
- æ— éœ€æ‰‹åŠ¨ä¸‹è½½
- é¦–æ¬¡å¯åŠ¨è‡ªåŠ¨ä¸‹è½½åˆ°ç¼“å­˜
- é€‚åˆå¼€å‘æµ‹è¯•

**ç¼ºç‚¹**:
- éœ€è¦ç½‘ç»œè®¿é—® HuggingFace
- å…¬å¸ä»£ç†å¯èƒ½å¯¼è‡´ä¸‹è½½å¤±è´¥
- å¤§æ¨¡å‹ä¸‹è½½æ—¶é—´é•¿

#### æ–¹æ¡ˆ B: æœ¬åœ°æ¨¡å‹æŒ‚è½½ (ç”Ÿäº§æ¨è)
```yaml
environment:
  - MODEL_NAME=Qwen/Qwen2.5-7B-Instruct
  - LOCAL_MODEL_PATH=/app/models/qwen
  - DEVICE=cpu
volumes:
  - C:/develop/Qwen2.5-7B-Instruct:/app/models/qwen:ro
```

**ä¼˜ç‚¹**:
- å®Œå…¨ç¦»çº¿è¿è¡Œï¼Œæ— éœ€ç½‘ç»œ
- é¿å…å…¬å¸ä»£ç†é—®é¢˜
- å¯åŠ¨é€Ÿåº¦å¿«ï¼ˆæ— éœ€ä¸‹è½½ï¼‰

**ç¼ºç‚¹**:
- éœ€è¦æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶
- éœ€è¦è¶³å¤Ÿç£ç›˜ç©ºé—´ï¼ˆQwen2.5-7B ~14GBï¼‰

### æ”¯æŒçš„æ¨¡å‹

#### å·²æµ‹è¯•æ¨¡å‹

| æ¨¡å‹ | å¤§å° | è®¾å¤‡ | å¯åŠ¨æ—¶é—´ | æ¨èåœºæ™¯ |
|------|------|------|----------|----------|
| **gpt2** | ~500MB | CPU | 30ç§’ | å¿«é€Ÿæµ‹è¯•ã€æ¶æ„éªŒè¯ |
| **Qwen/Qwen2.5-1.5B-Instruct** | ~3GB | CPU | 1-2åˆ†é’Ÿ | å¼€å‘ç¯å¢ƒã€è½»é‡éƒ¨ç½² |
| **Qwen/Qwen2.5-7B-Instruct** | ~14GB | CPU | 3-5åˆ†é’Ÿ | ç”Ÿäº§ç¯å¢ƒã€é«˜è´¨é‡æ¨ç† |
| **Qwen/Qwen2.5-7B-Instruct** | ~14GB | CUDA | 1-2åˆ†é’Ÿ | GPU åŠ é€Ÿæ¨ç† |

#### åˆ‡æ¢æ¨¡å‹æ–¹æ³•

**ä¸´æ—¶æµ‹è¯• (ä¸ä¿®æ”¹ä»£ç )**:
```bash
docker compose down model-service
export MODEL_NAME="Qwen/Qwen2.5-1.5B-Instruct"
docker compose up -d model-service
```

**æ°¸ä¹…ä¿®æ”¹**:
ç¼–è¾‘ `docker-compose.yml`:
```yaml
environment:
  - MODEL_NAME=Qwen/Qwen2.5-7B-Instruct
```

### é”™è¯¯å¤„ç†å’Œå¥åº·æ£€æŸ¥

#### å¥åº·æ£€æŸ¥å“åº”
```json
{
  "status": "healthy",
  "service": "model-service",
  "version": "0.1.0",
  "model_loaded": true,
  "model_name": "gpt2"
}
```

**çŠ¶æ€è¯´æ˜**:
- `status: "healthy"` - æ¨¡å‹å·²åŠ è½½ï¼Œå¯ä»¥å¤„ç†è¯·æ±‚
- `status: "unhealthy"` - æ¨¡å‹æœªåŠ è½½æˆ–åŠ è½½å¤±è´¥
- `model_loaded: true` - æ¨¡å‹å·²æˆåŠŸåŠ è½½åˆ°å†…å­˜
- `model_loaded: false` - æ¨¡å‹åŠ è½½å¤±è´¥æˆ–å°šæœªåŠ è½½

#### Kubernetes æ¢é’ˆé…ç½®

**Liveness Probe** (å­˜æ´»æ¢é’ˆ):
```yaml
livenessProbe:
  httpGet:
    path: /live
    port: 8004
  initialDelaySeconds: 10
  periodSeconds: 30
```

**Readiness Probe** (å°±ç»ªæ¢é’ˆ):
```yaml
readinessProbe:
  httpGet:
    path: /ready
    port: 8004
  initialDelaySeconds: 120  # æ¨¡å‹åŠ è½½æ—¶é—´
  periodSeconds: 10
```

---

## ğŸ“Š API ç¤ºä¾‹

### 1. å¥åº·æ£€æŸ¥

**è¯·æ±‚**:
```bash
curl http://localhost:8004/health
```

**å“åº”**:
```json
{
  "status": "healthy",
  "service": "model-service",
  "version": "0.1.0",
  "model_loaded": true,
  "model_name": "gpt2"
}
```

### 2. æ¨¡å‹ä¿¡æ¯

**è¯·æ±‚**:
```bash
curl http://localhost:8004/model/info
```

**å“åº”**:
```json
{
  "name": "gpt2",
  "path": "gpt2",
  "is_local": false,
  "type": "transformers",
  "device": "cpu",
  "max_length": 4096,
  "parameters": {
    "default_max_tokens": 512,
    "default_temperature": 0.7,
    "default_top_p": 0.9
  }
}
```

### 3. æ–‡æœ¬ç”Ÿæˆ

**è¯·æ±‚**:
```bash
curl -X POST http://localhost:8004/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Write a Python function to calculate fibonacci:",
    "max_tokens": 100,
    "temperature": 0.3,
    "top_p": 0.95
  }'
```

**å“åº”**:
```json
{
  "text": "\n\ndef fibonacci(n):\n    if n <= 1:\n        return n\n    else:\n        return fibonacci(n-1) + fibonacci(n-2)\n\nprint(fibonacci(10))",
  "prompt": "Write a Python function to calculate fibonacci:",
  "tokens_generated": 45,
  "finish_reason": "stop"
}
```

### 4. é€šè¿‡ API Gateway è°ƒç”¨ (éœ€è¦ JWT)

**è¯·æ±‚**:
```bash
# 1. è·å– JWT Token (å‡è®¾å·²æœ‰)
TOKEN="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."

# 2. è°ƒç”¨ç”Ÿæˆæ¥å£
curl -X POST http://localhost:8000/api/v1/generate \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hello!", "max_tokens": 20}'
```

---

## ğŸš€ éƒ¨ç½²æŒ‡å—

### å¿«é€Ÿå¯åŠ¨ (ä½¿ç”¨ gpt2)

```bash
# 1. è¿›å…¥é¡¹ç›®ç›®å½•
cd C:\develop\workflow-ai

# 2. æ„å»ºé•œåƒ
docker compose build model-service

# 3. å¯åŠ¨æœåŠ¡
docker compose up -d model-service

# 4. ç­‰å¾…æ¨¡å‹åŠ è½½ (1-2 åˆ†é’Ÿ)
docker compose logs -f model-service

# 5. æµ‹è¯•
curl http://localhost:8004/health
```

### ä½¿ç”¨ Qwen2.5-7B-Instruct

#### æ­¥éª¤ 1: ä¸‹è½½æ¨¡å‹

**PowerShell è„šæœ¬**:
```powershell
# åˆ›å»ºç›®å½•
New-Item -ItemType Directory -Path "C:\develop\Qwen2.5-7B-Instruct" -Force
cd C:\develop\Qwen2.5-7B-Instruct

# ä¸‹è½½æ–‡ä»¶ (å…± 12 ä¸ªæ–‡ä»¶, ~14GB)
$files = @(
    "config.json",
    "generation_config.json",
    "model.safetensors.index.json",
    "model-00001-of-00004.safetensors",  # ~5GB
    "model-00002-of-00004.safetensors",  # ~5GB
    "model-00003-of-00004.safetensors",  # ~5GB
    "model-00004-of-00004.safetensors",  # ~500MB
    "tokenizer.json",
    "tokenizer_config.json",
    "vocab.json",
    "merges.txt",
    "special_tokens_map.json"
)

$baseUrl = "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main"

foreach ($file in $files) {
    Write-Host "Downloading $file..."
    curl.exe -k -L -o $file "$baseUrl/$file"
}
```

#### æ­¥éª¤ 2: é…ç½® Docker Compose

ç¼–è¾‘ `docker-compose.yml`:
```yaml
model-service:
  environment:
    - MODEL_NAME=Qwen/Qwen2.5-7B-Instruct  # æ”¹ä¸º Qwen
    - LOCAL_MODEL_PATH=/app/models/qwen
  volumes:
    - model_cache:/app/cache
    - C:/develop/Qwen2.5-7B-Instruct:/app/models/qwen:ro  # å–æ¶ˆæ³¨é‡Š
```

#### æ­¥éª¤ 3: é‡æ–°æ„å»ºå’Œå¯åŠ¨

```bash
docker compose build model-service
docker compose up -d model-service

# ç­‰å¾… 3-5 åˆ†é’Ÿ (æ¨¡å‹åŠ è½½åˆ°å†…å­˜)
docker compose logs -f model-service
```

#### æ­¥éª¤ 4: éªŒè¯

```bash
# æ£€æŸ¥å¥åº·çŠ¶æ€
curl http://localhost:8004/health | jq '.'

# æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
curl http://localhost:8004/model/info | jq '.'

# æµ‹è¯•ç”Ÿæˆ
curl -X POST http://localhost:8004/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt":"ä½ å¥½", "max_tokens":50}' | jq '.'
```

---

## âš ï¸ å·²çŸ¥é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### é—®é¢˜ 1: å…¬å¸ä»£ç†å¯¼è‡´æ¨¡å‹ä¸‹è½½å¤±è´¥

**ç—‡çŠ¶**:
```
requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='huggingface.co', port=443)
```

**è§£å†³æ–¹æ¡ˆ**:
- ä½¿ç”¨æœ¬åœ°æ¨¡å‹æŒ‚è½½æ–¹æ¡ˆï¼ˆæ–¹æ¡ˆ Bï¼‰
- æ‰‹åŠ¨ä» Hugging Face ç½‘é¡µä¸‹è½½æ¨¡å‹æ–‡ä»¶
- ä½¿ç”¨ `curl -k` ç»•è¿‡ SSL è¯ä¹¦éªŒè¯

### é—®é¢˜ 2: å†…å­˜ä¸è¶³ (OOM)

**ç—‡çŠ¶**:
```
RuntimeError: DefaultCPUAllocator: not enough memory
```

**è§£å†³æ–¹æ¡ˆ**:
1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ (gpt2 æˆ– Qwen2.5-1.5B)
2. å¢åŠ  Docker å†…å­˜é™åˆ¶
3. ä½¿ç”¨ `low_cpu_mem_usage=True` (å·²å®ç°)
4. è®¾ç½®ç¯å¢ƒå˜é‡: `PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512`

### é—®é¢˜ 3: æ¨¡å‹åŠ è½½æ—¶é—´è¿‡é•¿

**ç—‡çŠ¶**:
å¥åº·æ£€æŸ¥è¶…æ—¶ï¼Œå®¹å™¨é‡å¯

**è§£å†³æ–¹æ¡ˆ**:
1. å»¶é•¿ healthcheck çš„ `start-period`:
```yaml
healthcheck:
  start-period: 300s  # ä» 120s å¢åŠ åˆ° 300s
```

2. ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹ (gpt2)
3. ä½¿ç”¨ SSD å­˜å‚¨æ¨¡å‹æ–‡ä»¶

### é—®é¢˜ 4: GPU ä¸å¯ç”¨ä½†é…ç½®äº† CUDA

**ç—‡çŠ¶**:
```
AssertionError: Torch not compiled with CUDA enabled
```

**è§£å†³æ–¹æ¡ˆ**:
ä»£ç å·²è‡ªåŠ¨å¤„ç†ï¼Œä¼šå›é€€åˆ° CPU:
```python
if self.device == "cuda" and not torch.cuda.is_available():
    print("CUDA not available, falling back to CPU")
    self.device = "cpu"
```

---

## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡

### å¯åŠ¨æ—¶é—´

| æ¨¡å‹ | è®¾å¤‡ | é¦–æ¬¡å¯åŠ¨ (å«ä¸‹è½½) | åç»­å¯åŠ¨ (ç¼“å­˜) |
|------|------|-------------------|-----------------|
| gpt2 | CPU | 2-3 åˆ†é’Ÿ | 30 ç§’ |
| Qwen2.5-1.5B | CPU | 5-8 åˆ†é’Ÿ | 1-2 åˆ†é’Ÿ |
| Qwen2.5-7B | CPU | 15-20 åˆ†é’Ÿ | 3-5 åˆ†é’Ÿ |
| Qwen2.5-7B | CUDA | 10-15 åˆ†é’Ÿ | 1-2 åˆ†é’Ÿ |

### æ¨ç†æ€§èƒ½

#### gpt2 (CPU)
- **Tokens/ç§’**: ~10-15 tokens/s
- **é¦–æ¬¡å“åº”**: <1 ç§’
- **å†…å­˜å ç”¨**: ~2GB

#### Qwen2.5-7B (CPU)
- **Tokens/ç§’**: ~2-5 tokens/s
- **é¦–æ¬¡å“åº”**: 2-3 ç§’
- **å†…å­˜å ç”¨**: ~16GB

#### Qwen2.5-7B (CUDA)
- **Tokens/ç§’**: ~50-100 tokens/s
- **é¦–æ¬¡å“åº”**: <500ms
- **æ˜¾å­˜å ç”¨**: ~14GB

---

## ğŸ”„ é›†æˆçŠ¶æ€

### ä¸å…¶ä»–æœåŠ¡çš„é›†æˆ

| æœåŠ¡ | çŠ¶æ€ | ç«¯ç‚¹ | è¯´æ˜ |
|------|------|------|------|
| **API Gateway** | âœ… å·²é›†æˆ | `/api/v1/generate`, `/api/v1/model/info` | JWT è®¤è¯ + é€Ÿç‡é™åˆ¶ |
| **Agent Orchestrator** | â³ å¾…é›†æˆ | - | Week 2 è®¡åˆ’ï¼šAgent è°ƒç”¨ Model Service |
| **Ingestion Service** | âŒ æœªé›†æˆ | - | æ— ä¾èµ–å…³ç³» |
| **Indexing Service** | âŒ æœªé›†æˆ | - | ç‹¬ç«‹è¿è¡Œ |
| **Metrics Service** | â³ å¾…é›†æˆ | - | Week 2 è®¡åˆ’ï¼šè®°å½•æ¨ç†æŒ‡æ ‡ |

### API Gateway è·¯ç”±

```go
// services/api-gateway/main.go
api := r.Group("/api/v1")
api.Use(middleware.Authenticate(cfg.JWTSecret))
{
    // Model Service
    api.POST("/generate", utils.ProxyToService(cfg.ModelServiceURL))
    api.GET("/model/info", utils.ProxyToService(cfg.ModelServiceURL+"/model/info"))
}
```

**ç‰¹æ€§**:
- âœ… JWT ä»¤ç‰ŒéªŒè¯
- âœ… é€Ÿç‡é™åˆ¶ (ä» Gateway é…ç½®ç»§æ‰¿)
- âœ… ç”¨æˆ·ä¸Šä¸‹æ–‡ä¼ é€’ (X-User-ID header)
- âœ… è¯·æ±‚/å“åº”æ—¥å¿—

---

## ğŸ“š åç»­æ”¹è¿›è®¡åˆ’

### Week 2 ä¼˜åŒ–

1. **vLLM é›†æˆ** (æ€§èƒ½æå‡)
   - ä½¿ç”¨ vLLM æ›¿ä»£åŸç”Ÿ Transformers
   - å®ç°æ‰¹é‡æ¨ç†
   - å¯ç”¨ PagedAttention ä¼˜åŒ–

2. **æµå¼å“åº”** (ç”¨æˆ·ä½“éªŒ)
   - å®ç° Server-Sent Events (SSE)
   - é€ token è¿”å›ç”Ÿæˆç»“æœ
   - å‡å°‘é¦–å­—å»¶è¿Ÿ

3. **æ¨¡å‹åˆ‡æ¢** (çµæ´»æ€§)
   - è¿è¡Œæ—¶åŠ¨æ€åŠ è½½æ¨¡å‹
   - æ”¯æŒå¤šæ¨¡å‹å¹¶è¡ŒæœåŠ¡
   - A/B æµ‹è¯•ä¸åŒæ¨¡å‹

4. **ç›‘æ§æŒ‡æ ‡** (å¯è§‚æµ‹æ€§)
   - æ¨ç†å»¶è¿Ÿåˆ†å¸ƒ (P50, P95, P99)
   - Tokens/ç§’ååé‡
   - æ¨¡å‹ GPU/CPU åˆ©ç”¨ç‡
   - è¯·æ±‚é˜Ÿåˆ—é•¿åº¦

### Month 2-3 é«˜çº§ç‰¹æ€§

1. **LoRA å¾®è°ƒ** (Week 5)
   - åœ¨ Qwen2.5-7B ä¸Šå¾®è°ƒåˆ†ç±»å™¨
   - ä½ç§©é€‚åº” (Low-Rank Adaptation)
   - æ•…éšœåˆ†ç±»ä¸“ç”¨æ¨¡å‹

2. **ç¼“å­˜å±‚** (Week 10)
   - ç›¸ä¼¼ prompt ç¼“å­˜
   - Redis å­˜å‚¨å¸¸è§å“åº”
   - å‡å°‘é‡å¤æ¨ç†æˆæœ¬

3. **è´Ÿè½½å‡è¡¡** (Week 11)
   - å¤šå‰¯æœ¬éƒ¨ç½²
   - æ™ºèƒ½è·¯ç”± (æ ¹æ® prompt é•¿åº¦)
   - GPU/CPU æ··åˆè°ƒåº¦

---

## ğŸ“ æŠ€æœ¯äº®ç‚¹

### 1. ç¦»çº¿è¿è¡Œèƒ½åŠ›
é€šè¿‡ `local_files_only=True` å’Œæœ¬åœ°æ¨¡å‹æŒ‚è½½ï¼Œå®Œå…¨æ— éœ€ç½‘ç»œè®¿é—®ï¼Œé€‚åˆå†…ç½‘éƒ¨ç½²ã€‚

### 2. è®¾å¤‡è‡ªé€‚åº”
è‡ªåŠ¨æ£€æµ‹ CUDA å¯ç”¨æ€§ï¼Œæ— ç¼å›é€€åˆ° CPUï¼Œå¼€å‘å’Œç”Ÿäº§ç¯å¢ƒé›¶é…ç½®å·®å¼‚ã€‚

### 3. å†…å­˜ä¼˜åŒ–
`low_cpu_mem_usage=True` å‚æ•°ä½¿å¤§æ¨¡å‹åŠ è½½æ—¶å†…å­˜å³°å€¼é™ä½ 30-40%ã€‚

### 4. æ¨¡å‹çµæ´»æ€§
æ”¯æŒä»»ä½• HuggingFace Transformers å…¼å®¹æ¨¡å‹ï¼Œæ— éœ€ä¿®æ”¹ä»£ç ã€‚

### 5. ç”Ÿäº§å°±ç»ª
å®Œæ•´çš„å¥åº·æ£€æŸ¥ã€Kubernetes æ¢é’ˆã€ä¼˜é›…å…³é—­ï¼Œç¬¦åˆäº‘åŸç”Ÿæœ€ä½³å®è·µã€‚

---

## ğŸ“ æäº¤è¯´æ˜

æœ¬æ¬¡æäº¤å®Œæˆäº† **Week 1 Day 7** çš„æ‰€æœ‰ç›®æ ‡ï¼š

- âœ… å®ç°å®Œæ•´çš„ LLM æ¨ç†æœåŠ¡
- âœ… æ”¯æŒæœ¬åœ°æ¨¡å‹å’Œ HuggingFace è‡ªåŠ¨ä¸‹è½½
- âœ… é›†æˆåˆ° API Gateway
- âœ… æä¾›å®Œæ•´çš„æµ‹è¯•å’Œéƒ¨ç½²è„šæœ¬
- âœ… è¯¦ç»†çš„æ–‡æ¡£å’Œæ•…éšœæ’æŸ¥æŒ‡å—

**Week 1 è¿›åº¦**: 7/7 å¤©å®Œæˆ (100%)

**ä¸‹ä¸€æ­¥**: Week 2 Day 1 - Agent Orchestrator é›†æˆå’Œ LangChain å·¥ä½œæµ

---

## ğŸ”— ç›¸å…³èµ„æº

- [HuggingFace Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers)
- [Qwen2.5 æ¨¡å‹å¡ç‰‡](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)
- [FastAPI æ–‡æ¡£](https://fastapi.tiangolo.com/)
- [Docker Compose æ–‡æ¡£](https://docs.docker.com/compose/)
- [vLLM å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/) (Week 2 è®¡åˆ’ä½¿ç”¨)

---

**æœ€åæ›´æ–°**: 2026-02-27  
**ä½œè€…**: Ren (AI Workflow é¡¹ç›®è´Ÿè´£äºº)
